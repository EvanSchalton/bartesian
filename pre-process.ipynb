{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "MODULE_DIR = \"/workspaces/bartesian\"\n",
    "\n",
    "if MODULE_DIR not in sys.path:\n",
    "    sys.path.append(MODULE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bartesian.barcodes import (\n",
    "    isolate_barcodes,\n",
    "    resize_barcodes,\n",
    "    barcode_to_widths,\n",
    "    normalize_barcodes,\n",
    "    BarcodeVocabulary,\n",
    "    draw_barcode,\n",
    ")\n",
    "\n",
    "from bartesian.datasheet import (load_data, DataSheetRecord)\n",
    "\n",
    "from bartesian.configs import (LIQUOR_ORDER, STRENGTH_ORDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = Path(\"/workspaces/bartesian/images/raw_images\")\n",
    "CROPPED_IMAGE_DIR = Path(\"/workspaces/bartesian/images/cropped_images\")\n",
    "RESIZED_IMAGE_DIR = Path(\"/workspaces/bartesian/images/resized_images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "isolate_barcodes(IMAGE_DIR, CROPPED_IMAGE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while True:\n",
    "    response = input(\"Deleted all invalid barcodes? (y/n)\").lower()\n",
    "    if response == \"n\":\n",
    "        raise Exception(\"Please delete all invalid barcodes before continuing\")\n",
    "    if response == \"y\":\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TARGET_BARCODE_RESOLUTION = (1024, 530)\n",
    "resize_barcodes(\n",
    "    CROPPED_IMAGE_DIR,\n",
    "    RESIZED_IMAGE_DIR,\n",
    "    resolution=TARGET_BARCODE_RESOLUTION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process each barcode image in the directory\n",
    "barcode_widths_dict: dict[str, list[int]] = {}\n",
    "barcodes = glob(os.path.join(RESIZED_IMAGE_DIR, '*'))\n",
    "for barcode_image in barcodes:\n",
    "    widths = barcode_to_widths(barcode_image)\n",
    "    barcode_widths_dict[barcode_image] = widths\n",
    "\n",
    "# barcode_widths_dict now contains the representation of each barcode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vocab_size in range(13,14):\n",
    "    test_size = normalize_barcodes(\n",
    "        barcode_widths_dict,\n",
    "        vocabulary_size=vocab_size\n",
    "    )\n",
    "    print(vocab_size)\n",
    "    draw_barcode(\n",
    "        test_size[\"barcodes\"][\"/workspaces/bartesian/images/resized_images/0[PXL_20240121_013857214.MP].jpg\"],\n",
    "        test_size[\"thickness\"],\n",
    "        .3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_record_json = load_data()\n",
    "all_records = {i[\"drink\"]: i for i in all_record_json}\n",
    "records = {i[\"drink\"]: i for i in all_record_json if i[\"image\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bartesian.ml.utilities import record_to_matrix\n",
    "from bartesian.datasheet import review_datasheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_datasheet(all_record_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bartesian.enums import Drink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drink_to_filename: dict[Drink, str] = {\n",
    "    Drink.SEX_ON_THE_BEACH: 'PXL_20240121_013747782.jpg',\n",
    "    Drink.PASSION_FRUIT_MARGARITA: 'PXL_20240121_013803629.MP.jpg',\n",
    "    Drink.COSMOPOLITAN: 'PXL_20240121_013815977.MP.jpg',\n",
    "    Drink.BEES_KNEES: 'PXL_20240121_013832187.MP.jpg',\n",
    "    Drink.PINEAPPLE_MARGARITA: 'PXL_20240121_013842468.MP.jpg',\n",
    "    Drink.ESPRESSO_MARTINI: 'PXL_20240121_013850719.MP.jpg',\n",
    "    Drink.WHISKEY_SOUR: 'PXL_20240121_013857214.MP.jpg',\n",
    "    Drink.SIDECAR: 'PXL_20240121_013909370.jpg',\n",
    "    Drink.BLACKBERRY_MARGARITA: 'PXL_20240121_184839130.jpg',\n",
    "    Drink.APPLE_PIE: 'PXL_20240121_013724688.jpg',\n",
    "    Drink.LEMON_DROP: 'PXL_20240121_184849045.jpg',\n",
    "    Drink.MANHATTAN: 'PXL_20240121_013733706.MP.jpg',\n",
    "    Drink.OLD_FASHIONED: 'PXL_20240121_184912979.jpg',\n",
    "}\n",
    "\n",
    "file_name_to_drink = {v:k for k,v in drink_to_filename.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barcode_results = normalize_barcodes(\n",
    "    barcode_widths_dict,\n",
    "    vocabulary_size=13\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "for k,v in barcode_results[\"barcodes\"].items():\n",
    "    try:    \n",
    "        target_file = Path(k).name.replace(\"0[\",\"\").replace(\"].jpg\",\".jpg\")\n",
    "        drink = file_name_to_drink[target_file]\n",
    "        print(target_file, drink)\n",
    "        training_data.append({\n",
    "            \"filename\": k,\n",
    "            \"input\": v,\n",
    "            \"output\": record_to_matrix(records[drink])\n",
    "        })\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "strengths: set[float] = set()\n",
    "for v in records.values():\n",
    "    for strength in STRENGTH_ORDER:\n",
    "        strengths.add(v[strength])\n",
    "\n",
    "strength_categories = dict(zip(range(len(strengths)), sorted(strengths)))\n",
    "rev_strength_categories = {v:k for k,v in strength_categories.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Reshape, Conv1D, Flatten, MaxPooling1D, Dropout,\n",
    "    Input, Concatenate\n",
    ")\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_length):\n",
    "    input_layer = Input(shape=(input_length, 1))\n",
    "\n",
    "    # Create conv layers with different kernel sizes\n",
    "    conv_layers = []\n",
    "    for kernel_size in range(1, 6):  # Kernel sizes from 1 to 5\n",
    "        conv_layer = Conv1D(filters=32, kernel_size=kernel_size, activation='relu', padding='same')(input_layer)\n",
    "        conv_layers.append(conv_layer)\n",
    "\n",
    "    # Concatenate the outputs of the conv layers\n",
    "    concatenated = Concatenate()(conv_layers)\n",
    "\n",
    "    # Flatten before passing to the fully connected layers\n",
    "    flattened = Flatten()(concatenated)\n",
    "\n",
    "    # Fully connected layers\n",
    "\n",
    "    dense = Dense(64, activation='relu')(flattened)\n",
    "    dropout = Dropout(0.5)(dense)\n",
    "\n",
    "\n",
    "    # final = Dense(20, activation='softmax')(dropout3)  # Assuming 20 units for output\n",
    "    final = Dense(20, activation='sigmoid')(dropout)  # Assuming 20 units for output\n",
    "    output = Reshape((5,4))(final)\n",
    "\n",
    "    # Create and compile the model\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    # model.compile(optimizer='adam', loss='categorical_crossentropy', learning_rate=0.001)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "INPUT_SIZE = 37\n",
    "\n",
    "random.shuffle(training_data)\n",
    "X = np.array([i[\"input\"] for i in training_data])\n",
    "Y = np.array([i[\"output\"] for i in training_data])\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=len(X), shuffle=True)  # Leave-One-Out Cross-Validation\n",
    "fold_no = 1\n",
    "for train_index, val_index in kf.split(X):\n",
    "    # Split data into training and validation sets\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    Y_train, Y_val = Y[train_index], Y[val_index]\n",
    "\n",
    "    # Create a new model for each fold\n",
    "    model = create_model(INPUT_SIZE)\n",
    "\n",
    "    # Train the model\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    model.fit(X_train, Y_train, epochs=EPOCHS)\n",
    "\n",
    "    # Evaluate the model\n",
    "    scores = model.evaluate(X_val, Y_val, verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores}')\n",
    "    fold_no += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To make a prediction:\n",
    "prediction = model.predict(np.array([training_data[0][\"input\"]]))\n",
    "predicted_matrix = prediction.reshape(5, 4)\n",
    "\n",
    "print(training_data[0]['filename'].replace(\"0-\", \"\"))\n",
    "# print(output_to_continuous(predicted_matrix, strength_categories))\n",
    "print(np.around(predicted_matrix, decimals=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
